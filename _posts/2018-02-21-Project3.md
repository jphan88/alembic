---
title: Project 3 McNulty
category: Projects
feature_image: "https://s3.ap-southeast-1.amazonaws.com/images.deccanchronicle.com/dc-Cover-lupa3v07ggq6gjq856d4rap9q3-20170131065003.Medi.jpeg"
---
I am rapidly approaching the half way mark at Metis. Time has flown faster than expected. Time flys when you're having fun and working ever so diligently on producing your first portfolio of data science projects.

<!-- more -->

I decided to predict the classification of H-1B visa petitions for my third project at Metis. I was capitvated to this topic because I have several friends who are working in the United States with a H-1B visa. The H-1B is a non-immigrant visa that allows US employers to temporarily employ foreign talent for specialty occupations. It is the responsibility for the employer to submit the H-1B petition.  The government sets the cap of visas to 65,000. When that cap is surpassed, the certified petitions will enter a random lottery to be selected for a H-1B  visa. Therefore I will solely be predicting whether the H-1B  petition will be certified or denied and not the actual granting of the H-1B visa.

I retrieved my dataset from Kaggle which can be seen below. It includes the case status: certified and denied. I had over 220K of unique employer names and over 1500 unique standard occupational codes. It would have taken too much computation power to convert the features as dummies so instead I transformed the features by finding the certified rate and total petitions submitted by employer name and occupational code. Further more I had to extract the state from worksite and convert it into dummies. My data set included prevailing wage, full time position, and the petitions from 2011 – 2016.
{% include figure.html image="/assets/Screen Shot 2018-02-20 at 5.17.12 PM.png" caption = "Kaggle.com" position="center" %}


{% include figure.html image="/assets/Screen Shot 2018-02-23 at 11.11.47 AM.png" caption = "Kaggle.com" position="center" %}
As you can see the data set is very imbalanced, I have 97% of  my data is made up of certified petitions. I will later go back and explain how I resolved this when building my models. Another thing you’ll notice as the years move forward the number of denied petitions decreases. This is a strong indicator that year will be an important feature in helping the model classify the petitions.

{% include figure.html image="/assets/Screen Shot 2018-02-23 at 11.13.35 AM.png" caption = "Kaggle.com" position="center" %}
I plotted the distribution of prevailing wage by denied and certified petitions, we can see that this will not be a strong feature as they over lap a lot but we can still see that as the wage goes down the denied rate increases and certified rates have higher wages.

These are the top 50 Employers submitting the most H-1B certified petitions.  80% of these petitions are for computer related jobs.
{% include figure.html image="/assets/Screen Shot 2018-02-23 at 11.13.43 AM.png" caption = "Kaggle.com" position="center" %}

Lastly this is the distribution of petitions by state. California is the leading petitioner at 18%, followed by Texas and New york. Washington is at 3%. All states with big tech hubs.
{% include figure.html image="/assets/Screen Shot 2018-02-23 at 11.13.52 AM.png" caption = "Kaggle.com" position="center" %}

Before I could begin modeling I had to address the unbalanced classes. I was curious in what would give the better results in terms of downsampling and using class weight and using both. Here is my control group where I did neither. 

{% include figure.html image="/assets/Screen Shot 2018-02-23 at 11.20.06 AM.png" caption = "Kaggle.com" position="center" %}

The accuracy rate which is the percent of predictions that are correct is almost 1 and this makes sense b/c 96% of my data is made up of certified petitions.  Precision which is the percent of positive predictions were correct and the recall which is the percent of positive cases the model caught is also almost 1 and again this makes sense.  F1 score is just as high because it’s the harmonic mean of the recall and precision. Looking at these numbers you can easily be deceived and think the model is amazing until you consider specificity measure which is the percent of negative cases the model caught which is at only 23%. This really shows the importance of balancing the classes. The next data set I downsampled the certified class and did not use class weight. The accuracy to f1 score did take a hit but specificity nearly triples. In this case we are decreasing type I errors. Ultimately just using class weight and not downsampling produced the best results, as it still uses all the data available while giving more power to the minority class. 

